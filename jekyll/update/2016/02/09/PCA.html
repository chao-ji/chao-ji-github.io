<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>PCA Explained | SuperComputer’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="PCA Explained" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In Machine Learning and Statistics, Principal Component Analysis (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix." />
<meta property="og:description" content="In Machine Learning and Statistics, Principal Component Analysis (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2016/02/09/PCA.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2016/02/09/PCA.html" />
<meta property="og:site_name" content="SuperComputer’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-02-09T23:42:01+08:00" />
<script type="application/ld+json">
{"description":"In Machine Learning and Statistics, Principal Component Analysis (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2016/02/09/PCA.html","headline":"PCA Explained","dateModified":"2016-02-09T23:42:01+08:00","datePublished":"2016-02-09T23:42:01+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2016/02/09/PCA.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="SuperComputer's Blog" /></head>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">SuperComputer&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">PCA Explained</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2016-02-09T23:42:01+08:00" itemprop="datePublished">Feb 9, 2016
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In Machine Learning and Statistics, <strong>Principal Component Analysis</strong> (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix.</p>

<h2 id="prerequisite">Prerequisite</h2>

<p>Here it is assumed that you have some basic understanding of Linear Algebra (e.g. matrix arithmetics, the geometric interpretation of inner product), Calculus (e.g. Lagrangian Multiplier) and Statistics (e.g. what is mean and variance). In addition, definitions of some math concepts and theorems (listed below) are needed to prove the properties of the resulting transformed data matrix.</p>

<h4 id="definition-1">Definition 1</h4>
<p>A scalar <script type="math/tex">\lambda</script> is called an <em>eigenvalue</em> of a <script type="math/tex">n \times n</script> matrix <script type="math/tex">\mathbf{A}</script> if there is a nontrivial (non-zero) solution <script type="math/tex">x</script> of <script type="math/tex">\mathbf{A} \mathbf{x} = \lambda \mathbf{x}</script>. Such an <script type="math/tex">\mathbf{x}</script> is called an <em>eigenvector</em> corresponding to the eigenvalue <script type="math/tex">\lambda</script>.</p>

<h4 id="definition-2">Definition 2</h4>
<p>A real-valued <script type="math/tex">n \times n</script> symmetric matrix <script type="math/tex">\mathbf{A}</script> is said to be <em>positive semidefinite</em> if the scalar <script type="math/tex">\mathbf{x}^{T} \mathbf{A} \mathbf{x}</script> is non-negative for every non-zero real-valued column vector <script type="math/tex">\mathbf{x}</script>.</p>

<h4 id="theorem-1">Theorem 1</h4>
<p>If <script type="math/tex">\mathbf{A}</script> is a real-valued <script type="math/tex">n \times n</script> symmetric matrix, then the eigenvectors corresponding to <em>different</em> eigenvalues must be orthogonal to each other.</p>

<p>[http://www.math.hawaii.edu/~lee/linear/eigen.pdf]</p>
<h4 id="theorem-2">Theorem 2</h4>
<p>A real-valued <script type="math/tex">n \times n</script> symmetric matrix is positive semidefinite if and only if all of its eigenvalues are non-negative.</p>

<p>[http://theanalysisofdata.com/probability/C4.html]</p>
<h2 id="the-math">The Math</h2>

<p>Let’s say we have a <script type="math/tex">n \times d</script> data matrix <script type="math/tex">\mathbf{X}</script> where <script type="math/tex">n</script> is the number of observations and <script type="math/tex">d</script> is the number of features — each observation is described by <script type="math/tex">d</script> attributes.</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathbf{X} =
 \begin{pmatrix}
  x_{11} & x_{12} & \cdots & x_{1d} \\
  x_{21} & x_{22} & \cdots & x_{2d} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n1} & x_{n2} & \cdots & x_{nd} 
 \end{pmatrix} %]]></script>

<p>Now we wish to describe each observation by a single attribute. In other words, the <script type="math/tex">n \times d</script> matrix <script type="math/tex">X</script> will be turned into a <script type="math/tex">n \times 1</script> column matrix (i.e. vector), and we want the variance of the sample composed of the <script type="math/tex">n</script> elements from this vector to be as large as possible.</p>

<p align="center"><img src="/assets/projection.pdf" width="400" /><br />Project points from 2-D space to 1-D space</p>

<p>One way to do this (in the case of PCA) is to project the collection of <script type="math/tex">n</script> points in <script type="math/tex">d</script>-dimensional space into 1-dimensional space, or equivalently right-multiply <script type="math/tex">\mathbf{X}</script> with a column vector <script type="math/tex">\alpha</script></p>

<script type="math/tex; mode=display">\boldsymbol \alpha= \left( a_{1}, a_{2}, \cdots, a_{d} \right)^{T}</script>

<p>giving rise to the <script type="math/tex">n \times 1</script> transformed data matrix <script type="math/tex">\mathbf{Y}</script></p>

<script type="math/tex; mode=display">\mathbf{Y} = \mathbf{X} \boldsymbol \alpha = 
	\begin{pmatrix}
		x_{11}a_{1} + x_{12}a_{2} + \cdots + x_{1d}a_{d}\\
		x_{21}a_{1} + x_{22}a_{2} + \cdots + x_{2d}a_{d}\\
		\vdots \\
		x_{n1}a_{1} + x_{n2}a_{2} + \cdots + x_{nd}a_{d} 
	\end{pmatrix} =
	\begin{pmatrix}
		\sum_{j=1}^{d}x_{1j}a_{j} \\
		\sum_{j=1}^{d}x_{2j}a_{j} \\
		\vdots \\
		\sum_{j=1}^{d}x_{nj}a_{j}
	\end{pmatrix} =
	\begin{pmatrix}
	y_{1} \\
	y_{2} \\
	\vdots \\
	y_{n}
	\end{pmatrix}</script>

<p><strong>Note</strong>: For mathmatical convenience, the data matrix <script type="math/tex">\mathbf{X}</script> is assumed to have been <strong>centered</strong> (each column sums to one): <script type="math/tex">\sum_{i=1}^{n} x_{ij} = 0</script>, where <script type="math/tex">j=1 \dotsc d</script></p>

<p>In case <script type="math/tex">\mathbf{X}</script> had not been centered, you can simply subtract the column-mean from each element in each column.</p>

<p>Given that <script type="math/tex">\mathbf{X}</script> has been centered, the transformed data matrix <script type="math/tex">\mathbf{Y}</script> will be centered as well:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathbf{\bar{Y}} &=\frac{1}{n} \sum_{i=1}^{n} y_{i} = \frac{1}{n} \sum_{i=1}^{n} \left(  \sum_{j=1}^{d} x_{ij}a_{j}  \right) = \frac{1}{n} \sum_{j=1}^{d}  \left(  \sum_{i=1}^{n} x_{ij}a_{j}  \right) \\
        &=  \frac{1}{n} \sum_{j=1}^{d}  a_{j}\left(  \sum_{i=1}^{n} x_{ij}  \right) =  \frac{1}{n} \sum_{j=1}^{d}  a_{j}\cdot 0 = 0
\end{align} %]]></script>

<p>Remember we wanted to make the variance of the sample composed of the <script type="math/tex">n</script> elements from <script type="math/tex">\mathbf{Y}</script> to be as large as possible. In other words, we want to maximize <script type="math/tex">\textrm{Var}(\mathbf{Y})</script> (over the space of all <script type="math/tex">\boldsymbol \alpha</script>), which can be expressed in terms of <script type="math/tex">\mathbf{X}</script> and <script type="math/tex">\boldsymbol \alpha</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\textrm{Var}(\mathbf{Y}) & = \frac{1}{n - 1} \sum_{i=1}^{n} \left( y_{i} - \mathbf{\bar{Y}} \right)^{2}=\frac{1}{n -1}\sum_{i=1}^{n}y_{i}^{2} \\ 
  & = \frac{1}{n - 1}\mathbf{Y}^{T}\mathbf{Y}=\boldsymbol\alpha^{T}\left( \frac{1}{n-1} \mathbf{X}^{T}\mathbf{X} \right) \boldsymbol\alpha
\end{align} %]]></script>

<p>Here we can see that <script type="math/tex">\textrm{Var}(\mathbf{Y})</script> is proportional to the norm of <script type="math/tex">\mathbf{Y}</script>, which in turn depends on the norm of <script type="math/tex">\boldsymbol \alpha</script>. If we were to allow the norm of <script type="math/tex">\boldsymbol \alpha</script> to be unbounded, the <script type="math/tex">\textrm{Var}(\mathbf{Y})</script> could be arbitrarily large!</p>

<p>To fix this we need to put some constraint on <script type="math/tex">\boldsymbol \alpha</script> — we make it a unit-length vector: <script type="math/tex">\boldsymbol\alpha^{T} \boldsymbol\alpha = \sum_{j=1}^{d}a_{j}^{2} = 1</script></p>

<p>So this turns the original function optimization problem into one with an equality constraint, which can be solved using <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrangian Multiplier</a>:</p>

<script type="math/tex; mode=display">L(\boldsymbol\alpha, \lambda) = \boldsymbol\alpha^{T} \left( \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X} \right) \boldsymbol\alpha - \lambda \left(\boldsymbol\alpha^{T} \boldsymbol\alpha - 1\right)</script>

<p>Taking the derivative of the Lagrangian function <script type="math/tex">L</script> with respect to <script type="math/tex">\boldsymbol \alpha</script> and set it to zero,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial}{\partial \boldsymbol\alpha} L(\boldsymbol\alpha, \lambda) & =  \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X} \boldsymbol\alpha + \frac{1}{n - 1} \left( \mathbf{X}^{T} \mathbf{X} \right)^{T} \boldsymbol\alpha - \lambda \cdot 2 \boldsymbol \alpha \\ 
& = \frac{2}{n - 1} \mathbf{X}^{T} \mathbf{X} \cdot \boldsymbol\alpha - 2 \lambda \cdot \boldsymbol\alpha = 0
\end{align} %]]></script>

<p>we end up with the condition that <script type="math/tex">\boldsymbol \alpha</script> must statisfy:</p>

<p> </p>

<script type="math/tex; mode=display">\boldsymbol\Sigma \boldsymbol\alpha = \lambda \boldsymbol\alpha</script>

<p>where <script type="math/tex">\boldsymbol\Sigma = \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X}</script></p>

<p> </p>

<p>According to <strong>Defitition 1</strong>, <script type="math/tex">\boldsymbol \alpha</script> must be an eigenvector of <script type="math/tex">\boldsymbol \Sigma</script> corresponding to eigenvalue <script type="math/tex">\lambda</script></p>

<p>We can further show that the variance of <script type="math/tex">\mathbf{Y}</script> happens to be equal to <script type="math/tex">\lambda</script>:</p>

<script type="math/tex; mode=display">\textrm{Var}(\mathbf{Y}) = \boldsymbol\alpha^{T}\left( \frac{1}{n-1} \mathbf{X}^{T}\mathbf{X} \right) \boldsymbol\alpha = \boldsymbol\alpha^{T} \boldsymbol\Sigma \boldsymbol\alpha = \boldsymbol\alpha^{T} \lambda \boldsymbol\alpha = \lambda \cdot 1 = \lambda</script>

<p> </p>

<p>The <script type="math/tex">d\times d</script> matrix <script type="math/tex">\boldsymbol \Sigma</script> (i.e. the covariance matrix of <script type="math/tex">\mathbf{X}</script>) has the following properties:</p>

<ul>
  <li>
    <p><script type="math/tex">\boldsymbol\Sigma = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X}</script> is symmetric: <script type="math/tex">\boldsymbol \Sigma ^{T} = \left( \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} \right)^{T} = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} = \boldsymbol\Sigma</script></p>
  </li>
  <li>
    <p><script type="math/tex">\boldsymbol\Sigma = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X}</script> is semi-positive definite: For any real-valued non-zero vector <script type="math/tex">\mathbf{w} = \left(w_{1}, w_{2}, \cdots, w_{d} \right)^{T}</script>,</p>
  </li>
</ul>

<script type="math/tex; mode=display">\mathbf{w}^{T} \boldsymbol\Sigma \mathbf{w} = \mathbf{w}^{T} \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} \mathbf{w} = \frac{1}{n-1} \left( \mathbf{X} \mathbf{w} \right)^{T} \mathbf{X} \mathbf{w} = \frac{1}{n-1} \Vert \mathbf{X} \mathbf{w} \Vert ^{2} \ge 0</script>

<p> </p>

<p>Suppose <script type="math/tex">\lambda_{1}, \lambda_{2}, \cdots, \lambda_{d}</script> are the eigenvalues of <script type="math/tex">\boldsymbol \Sigma</script>, and <script type="math/tex">\{\boldsymbol \alpha_{1i_{1}}\}, \{\boldsymbol \alpha_{2i_{2}}\}, \cdots, \{\boldsymbol \alpha_{di_{d}}\}</script> are the sets of eigenvectors corresponding to each eigenvalue. Then <script type="math/tex">\lambda</script> must be one of the eigenvalues, and <script type="math/tex">\boldsymbol \alpha</script> must be an eigenvector corresponding to <script type="math/tex">\lambda</script>.</p>

<p>According to <strong>Theorem 1</strong>, we know that eigenvectors <script type="math/tex">\{\boldsymbol \alpha_{1i_{1}}\}, \{\boldsymbol \alpha_{2i_{2}}\}, \cdots, \{\boldsymbol \alpha_{di_{d}}\}</script> are orthogonal to each other.
<br />
According to <strong>Theorem 2</strong>, we know that the eigenvalues are non-negative — <script type="math/tex">\lambda_{1} \ge 0, \lambda_{2} \ge 0, \cdots, \lambda_{d} \ge 0</script></p>

<p> </p>

<p>Based on the properties of <script type="math/tex">\boldsymbol\Sigma</script>, <script type="math/tex">\boldsymbol\alpha</script>, <script type="math/tex">\lambda</script>, and <script type="math/tex">\textrm{Var}(\mathbf{Y})</script>, we can apply PCA on <script type="math/tex">\mathbf{X}</script> in the following steps:</p>

<ol>
  <li>Compute the covariance matrix <script type="math/tex">\boldsymbol\Sigma = \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X}</script>, where <script type="math/tex">\mathbf{X}</script> should have been centered.</li>
  <li>Find the eigenvalues <script type="math/tex">\lambda_{1}, \lambda_{2}, \cdots, \lambda_{d}</script> of <script type="math/tex">\boldsymbol\Sigma</script>, as well as the corresponding eigenvectors <script type="math/tex">\boldsymbol \alpha_{1}, \boldsymbol \alpha_{2}, \cdots, \boldsymbol \alpha_{d}</script>. The eigenvalues should be in descending order: <script type="math/tex">\lambda_{1} \ge \lambda_{2} \ge \cdots \ge \lambda_{d}</script></li>
  <li>Determine the number of features <script type="math/tex">k</script> to keep in the transformed matrix (<script type="math/tex">1 \le k \le d</script>).</li>
  <li>Build the <script type="math/tex">d \times k</script> matrix <script type="math/tex">M = \left( \boldsymbol \alpha_{1}, \boldsymbol \alpha_{2}, \cdots, \boldsymbol \alpha_{k} \right)</script></li>
  <li>Compute the transformed matrix <script type="math/tex">\mathbf{X}_{pca} = \mathbf{X}M = \left(\mathbf{X\alpha_{1}}, \mathbf{X\alpha_{2}}, \cdots, \mathbf{X\alpha_{k}} \right)</script>.</li>
</ol>

<p> </p>

<p>And we can prove that the columns of <script type="math/tex">\mathbf{X}_{pca}</script> have the properties that we desired:</p>
<ul>
  <li><script type="math/tex">\mathbf{X\alpha_{1}}</script> has the largest variance <script type="math/tex">\lambda_{1}</script>, followed by <script type="math/tex">\mathbf{X\alpha_{2}}</script> with variance <script type="math/tex">\lambda_{2}</script>, and so on.</li>
  <li>The columns of <script type="math/tex">\mathbf{X\alpha_{1}}</script> are orthogonal with each other — Consider <script type="math/tex">X\boldsymbol\alpha_{i}</script> and <script type="math/tex">X\boldsymbol\alpha_{j}</script> where <script type="math/tex">% <![CDATA[
1 \le i < j \le d %]]></script>). It follows that</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \left( X\boldsymbol\alpha_{i} \right) ^{T} X\boldsymbol\alpha_{j} & = \boldsymbol\alpha_{i}^{T} X^{T} X \boldsymbol\alpha_{j} = (n-1)\boldsymbol\alpha_{i}^{T} \boldsymbol\Sigma \boldsymbol\alpha_{j}  =(n-1)\boldsymbol\alpha_{i}^{T} \lambda_{j} \boldsymbol\alpha_{j} \\ &=(n-1)\lambda_{j} \boldsymbol\alpha_{i}^{T}  \boldsymbol\alpha_{j} = (n-1)\lambda_{j} \cdot 0 = 0 \end{align} %]]></script>

<h2 id="example">Example</h2>

<p>We conclude our discussion with an example where we’ll generate a synthetic dataset and apply PCA as prescribed above.</p>

<p>First we will generate 2-D dataset with covariance matrix <script type="math/tex">% <![CDATA[
\begin{pmatrix}0.1 & 0.0 \\ 0.0 & 1.0 \end{pmatrix} %]]></script> and mean <script type="math/tex">(-1, 1)</script> (shown in the figure below):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="k">def</span> <span class="nf">generate_2Ddata</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
  <span class="n">rot_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> 
                      <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>
  <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rot_mat</span><span class="p">)</span> <span class="o">+</span> <span class="n">offset</span>

<span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_2Ddata</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
</code></pre></div></div>

<p>Compute covariance matrix of a centered data matrix</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Center the data 
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Compute covariance matrix
</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Find the eigenvalues and eigenvectors, and sort them in descending order of eigenvalues.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># eigval[i] is the ith eigenvalue
# eigvec[:, i] is the eigenvector corresponding to eigval[i]
</span><span class="n">eigval</span><span class="p">,</span> <span class="n">eigvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">eigval</span><span class="p">)</span>

<span class="n">eigvec</span> <span class="o">=</span> <span class="n">eigvec</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">]</span>
</code></pre></div></div>

<p>For now we’ll keep all principle components, but you can keep the top <code class="highlighter-rouge">k</code> components with largest variance:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eigvec</span> <span class="o">=</span> <span class="n">eigvec</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
</code></pre></div></div>

<p>Apply PCA as a linear transformation on <code class="highlighter-rouge">X</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_pca</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigvec</span><span class="p">)</span>
</code></pre></div></div>

<p>Display PCA-transfomred data matrix in scatter plot</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'1st Principal Component'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'2nd Principal Component'</span><span class="p">)</span>
</code></pre></div></div>
<p align="center">
<img src="/assets/pca.pdf" width="1200" />
<br />
Left: Original, uncentered. Middle: Centered. Right: PCA-transformed
</p>


  </div><a class="u-url" href="/jekyll/update/2016/02/09/PCA.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">SuperComputer&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">SuperComputer&#39;s Blog</li><li><a class="u-email" href="mailto:ji.chao.stern@gmail.com">ji.chao.stern@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/chao-ji"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">chao-ji</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Practice. Research. Learn.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
