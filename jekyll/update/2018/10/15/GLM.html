<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>From Likelihood Maximization to Loss Minimization | SuperComputer’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="From Likelihood Maximization to Loss Minimization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Loss function is an important component for almost all Machine Learning models, especially neural networks. It compares the prediction that comes out of the model with the groundtruth, and determines how dissimilar they are (i.e. how bad the prediction is). For example, a ConvNet-based object detector performs both the classification task and the regression task, predicting a vector holding the confidence scores corresponding to object classes, and another vector describing how the prior box’s geometric shape should be refined so it can tightly bound the object instance. To measure the quality of the predictions, we compute the cross-entropy loss (Sigmoid or Softmax) and Huber loss (a variant of squared error loss) between the predictions and their respective groundtruths, and we tune the neural network weights to minimize the losses." />
<meta property="og:description" content="Loss function is an important component for almost all Machine Learning models, especially neural networks. It compares the prediction that comes out of the model with the groundtruth, and determines how dissimilar they are (i.e. how bad the prediction is). For example, a ConvNet-based object detector performs both the classification task and the regression task, predicting a vector holding the confidence scores corresponding to object classes, and another vector describing how the prior box’s geometric shape should be refined so it can tightly bound the object instance. To measure the quality of the predictions, we compute the cross-entropy loss (Sigmoid or Softmax) and Huber loss (a variant of squared error loss) between the predictions and their respective groundtruths, and we tune the neural network weights to minimize the losses." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/10/15/GLM.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/10/15/GLM.html" />
<meta property="og:site_name" content="SuperComputer’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-15T13:13:12+08:00" />
<script type="application/ld+json">
{"description":"Loss function is an important component for almost all Machine Learning models, especially neural networks. It compares the prediction that comes out of the model with the groundtruth, and determines how dissimilar they are (i.e. how bad the prediction is). For example, a ConvNet-based object detector performs both the classification task and the regression task, predicting a vector holding the confidence scores corresponding to object classes, and another vector describing how the prior box’s geometric shape should be refined so it can tightly bound the object instance. To measure the quality of the predictions, we compute the cross-entropy loss (Sigmoid or Softmax) and Huber loss (a variant of squared error loss) between the predictions and their respective groundtruths, and we tune the neural network weights to minimize the losses.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/10/15/GLM.html","headline":"From Likelihood Maximization to Loss Minimization","dateModified":"2018-10-15T13:13:12+08:00","datePublished":"2018-10-15T13:13:12+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/10/15/GLM.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="SuperComputer's Blog" /></head>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">SuperComputer&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">From Likelihood Maximization to Loss Minimization</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-15T13:13:12+08:00" itemprop="datePublished">Oct 15, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Loss function is an important component for almost all Machine Learning models, especially neural networks. It compares the <em>prediction</em> that comes out of the model with the <em>groundtruth</em>, and determines how dissimilar they are (i.e. how bad the prediction is). For example, a ConvNet-based object detector performs both the <strong>classification</strong> task and the <strong>regression</strong> task, predicting a vector <script type="math/tex">p_{cls}</script> holding the confidence scores corresponding to object classes, and another vector <script type="math/tex">p_{loc}</script> describing how the prior box’s geometric shape should be refined so it can tightly bound the object instance. To measure the quality of the predictions, we compute the cross-entropy loss (Sigmoid or Softmax) and Huber loss (a variant of squared error loss) between the predictions and their respective groundtruths, and we tune the neural network weights to minimize the losses.</p>

<p>You may legitmately ask why cross-entropy loss and squared-error loss are chosen for classification and regression tasks. In this post I’ll try to justify the choices by providing a probabilistic interpretation of different losss functions. To simplify the disscussion, I use the simplest form of neural network, one that just performs the affine transformation on the input (i.e. without nonlinearity), as the running example.</p>

<h3 id="maximize-likelihood-function">Maximize Likelihood Function</h3>


  </div><a class="u-url" href="/jekyll/update/2018/10/15/GLM.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">SuperComputer&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">SuperComputer&#39;s Blog</li><li><a class="u-email" href="mailto:ji.chao.stern@gmail.com">ji.chao.stern@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/chao-ji"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">chao-ji</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Democratize ML </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
